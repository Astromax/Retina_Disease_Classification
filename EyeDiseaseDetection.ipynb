{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EyeDiseaseDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf4HFUcV/9kxSQaSrrlzMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Astromax/Retina_Disease_Classification/blob/main/EyeDiseaseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpAakRV1_Rmr"
      },
      "source": [
        "The purpose of this is to determine whether or not a retina is healthy, and, if not, which of six different diseases it has. The data for this comes from a Kaggle competition, https://www.kaggle.com/c/vietai-advance-course-retinal-disease-detection/overview.\n",
        " There are 3,435 training images, with the following label breakdown: opacity - 0, diabetic retinopathy - 1, glaucoma - 2, macular edema - 3, macular degeneration - 4, retinal vascular occlusion - 5, and normal - 6.\n",
        "An important note: unlike an ordinary classification problem, in this case a retina may be afflicted with more than one ailment.  Because of this, the activation function for the final layer is sigmoid, instead of the usual softmax, effectively providing a probability for each label independent of the others.\n",
        "\n",
        "The data has been uploaded to my Google Drive for easy access & processing, if anyone reading this wants to replicate it just change the target directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mkBK6Qt_Ntr",
        "outputId": "bb8c327c-361d-4328-8de2-8459f8c70254"
      },
      "source": [
        "#First let's mount the Drive so we can access the data...\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COzRUZA0_mcA"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/EyeDiseaseDetection/')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYSa5OXB_ppC"
      },
      "source": [
        "from resnet_module_helpers_tf import *\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import multilabel_confusion_matrix as MLCM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.activations import relu\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, MaxPool2D, Dense, Dropout, Input, BatchNormalization, Activation\n",
        "from keras.layers.experimental.preprocessing import RandomCrop, RandomFlip, RandomRotation\n",
        "from tqdm import tqdm\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqjiKRuyAbWp"
      },
      "source": [
        "#Establish location of data\n",
        "data_dir = './Data/Data/train'\n",
        "image_dir = data_dir+'/train'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbTml54JAjYT",
        "outputId": "75cdddb2-82a8-4ec3-e7c2-b1f569cb20d2"
      },
      "source": [
        "filenames = os.listdir(image_dir)\n",
        "print('Number of files: ', len(filenames))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of files:  3435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAzb5p3LCuCX"
      },
      "source": [
        "Before doing the heavy-duty analysis, let's first make sure there are no anomalies.  A retina can have multiple diseases simultaneously, which means this is not a simple image classification problem.  However, there is also a seventh label, 'Normal', corresponding to a healthy retina.  If a retina has a disease it is, by definition, unhealthy, therefore any images with a one in the 'Normal' column *and any other* is an anomaly, which should be excluded from the analysis.  Similarly, an image with all zeros, corresponding to either a mislabel or a retina with a disease not in this set, is not useful for this analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiK_WmkdBsAo",
        "outputId": "4aa723c0-fd60-4184-a466-3f04ee4e60bb"
      },
      "source": [
        "#Let's check for anomalies and see how many of each disease type there are....\n",
        "labels_df = pd.read_csv(data_dir+'/train.csv')\n",
        "print('Number of all-zero anomalies: ', labels_df[(labels_df['opacity'] == 0) & (labels_df['diabetic retinopathy']==0)\n",
        "& (labels_df['glaucoma'] == 0) & (labels_df['macular edema'] == 0) & (labels_df['macular degeneration'] == 0) \n",
        "& (labels_df['retinal vascular occlusion'] == 0) & (labels_df['normal'] == 0)].count())\n",
        "\n",
        "print('Number of inconsistencies: ', labels_df[((labels_df['opacity'] == 1) | (labels_df['diabetic retinopathy']==1)\n",
        "| (labels_df['glaucoma'] == 1) | (labels_df['macular edema'] == 1) | (labels_df['macular degeneration'] == 1) \n",
        "| (labels_df['retinal vascular occlusion'] == 1)) & (labels_df['normal'] == 1)].count())\n",
        "\n",
        "print('Opacity count: ')\n",
        "print(labels_df['opacity'].value_counts())\n",
        "print('Diabetic Retinopathy count: ')\n",
        "print(labels_df['diabetic retinopathy'].value_counts())\n",
        "print('Glaucoma count: ')\n",
        "print(labels_df['glaucoma'].value_counts())\n",
        "print('Macular Edema count:')\n",
        "print(labels_df['macular edema'].value_counts())\n",
        "print('Macular Degeneration count:')\n",
        "print(labels_df['macular degeneration'].value_counts())\n",
        "print('Retinal Vascular Occlusion count: ')\n",
        "print(labels_df['retinal vascular occlusion'].value_counts())\n",
        "print('Normal count: ')\n",
        "print(labels_df['normal'].value_counts())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of all-zero anomalies:  filename                      0\n",
            "opacity                       0\n",
            "diabetic retinopathy          0\n",
            "glaucoma                      0\n",
            "macular edema                 0\n",
            "macular degeneration          0\n",
            "retinal vascular occlusion    0\n",
            "normal                        0\n",
            "dtype: int64\n",
            "Number of inconsistencies:  filename                      0\n",
            "opacity                       0\n",
            "diabetic retinopathy          0\n",
            "glaucoma                      0\n",
            "macular edema                 0\n",
            "macular degeneration          0\n",
            "retinal vascular occlusion    0\n",
            "normal                        0\n",
            "dtype: int64\n",
            "Opacity count: \n",
            "0    1902\n",
            "1    1533\n",
            "Name: opacity, dtype: int64\n",
            "Diabetic Retinopathy count: \n",
            "0    2680\n",
            "1     755\n",
            "Name: diabetic retinopathy, dtype: int64\n",
            "Glaucoma count: \n",
            "0    2838\n",
            "1     597\n",
            "Name: glaucoma, dtype: int64\n",
            "Macular Edema count:\n",
            "0    2919\n",
            "1     516\n",
            "Name: macular edema, dtype: int64\n",
            "Macular Degeneration count:\n",
            "0    2861\n",
            "1     574\n",
            "Name: macular degeneration, dtype: int64\n",
            "Retinal Vascular Occlusion count: \n",
            "0    2995\n",
            "1     440\n",
            "Name: retinal vascular occlusion, dtype: int64\n",
            "Normal count: \n",
            "0    2910\n",
            "1     525\n",
            "Name: normal, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsqGS4XzDtZY"
      },
      "source": [
        "**TO DO**: Some EDA to see what kinds of correlations there are between the different diseases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAvddh6AO91w"
      },
      "source": [
        "To streamline the process of entering in the labels for training, we first create a new column consolidating the entries in the seven main columns into a column of 7-tuples.  This is the \"y\" that the model is learning from and trying to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaJHzjxkD3jQ"
      },
      "source": [
        "#Convert the individual label values to int type, add in a multi-hot vector label\n",
        "MH = list(zip(labels_df['opacity'], labels_df['diabetic retinopathy'], labels_df['glaucoma'], labels_df['macular edema'],\n",
        "                                  labels_df['macular degeneration'], labels_df['retinal vascular occlusion'], labels_df['normal']))\n",
        "labels_df['Multi_Hot'] = MH"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_kqvkRyD9TQ",
        "outputId": "2ce13319-5b44-4e86-ee11-09ff542a0333"
      },
      "source": [
        "y = labels_df['Multi_Hot']\n",
        "y = np.array([y.iloc[i] for i in range(len(y))])\n",
        "print('Length of y: ', len(y))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of y:  3435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RdPGKwOP3iI"
      },
      "source": [
        "Here we load in the full set of images.  tqdm produces the progress bar and is not strictly necessary, but with this many images it can be nice to know how much longer to expect.  Google Colab has quite a bit of variability, in some experiments it loaded the full dataset in under 10 minutes, while other times it took more than half an hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wAhZNd4EQSn"
      },
      "source": [
        "train_images = []\n",
        "for i in tqdm(range(labels_df.shape[0])):\n",
        "    img = image.load_img(image_dir+'/'+ labels_df['filename'][i], target_size=(224,224,3), grayscale=False)\n",
        "    img = image.img_to_array(img)\n",
        "    img = img/255\n",
        "    train_images.append(img)\n",
        "\n",
        "X = np.array(train_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu3ZUSXEERui"
      },
      "source": [
        "#Perform the train_test split.  Fix random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=37, test_size=0.3)\n",
        "print('Length of X_train: ', len(X_train))\n",
        "print('Length of X_test: ', len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGU53ndUEYZa"
      },
      "source": [
        "#Specifying the batch_size hyperparameter.\n",
        "#I have heard it claimed that batch_size larger than 32 does not improve performance (see https://arxiv.org/pdf/1804.07612.pdf),\n",
        "#so I'm going to hold batch_size to 16 here just to be safe. \n",
        "train_batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-zanrNQEocI"
      },
      "source": [
        "We will want to augment the training set. For these images, vertical & horizontal flips as well as random rotations are valid transformations. Manipulating the colors is probably not a valid transformation as that might plausibly change the actual meaning, although I could be mistaken in this.\n",
        "The Keras ImageDataGenerator also has shear & shift parameters, which should be valid augmentations for this sort of data as long as the parameters aren't too large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlHNjiG0EqsE"
      },
      "source": [
        "#Hyperparameters: rotation_angle, width/height_shift_range, shear\n",
        "\n",
        "# Create a Data Generator\n",
        "data_gen_args = dict( \n",
        "    rotation_range=45,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.2,\n",
        "    channel_shift_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='constant',\n",
        "    data_format=\"channels_last\",\n",
        ")\n",
        "image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "image_datagen.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zl6vftbEwYD"
      },
      "source": [
        "train_generator = image_datagen.flow(X_train, y_train, batch_size=train_batch_size)\n",
        "val_generator = image_datagen.flow(X_test, y_test, batch_size=train_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU51mLy_E3H9"
      },
      "source": [
        "ResNets are the name of the game for this project, so let's start by defining an n-convolutional-layer resnet component. The original ResNet set n=2, but I've decided to experiment with adding more layers to each component.\n",
        "Acknowledgement: this code is based on the example code found here https://d2l.ai/chapter_convolutional-modern/resnet.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KvcMKo3FHv2"
      },
      "source": [
        "This is where the resnet module helpers come in: GeneralResnetBlock is defined in that function, it determines the complexity of the individual resnet blocks.  For the first model, we are using four 3-layer core modules, with a GoogLeNet head and tail bracketing the resnet blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1okLsW0E9md"
      },
      "source": [
        "def model_one():\n",
        "    return Sequential([\n",
        "           Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
        "           BatchNormalization(),\n",
        "           tf.keras.layers.Activation('relu'),\n",
        "           tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),      \n",
        "           GeneralResnetBlock(3, 64, 2, first_block=True),\n",
        "           GeneralResnetBlock(3, 128, 2),\n",
        "           GeneralResnetBlock(3, 256, 2),\n",
        "           GeneralResnetBlock(3, 512, 2),\n",
        "           tf.keras.layers.GlobalAvgPool2D(),\n",
        "           tf.keras.layers.Dense(7, activation='sigmoid')\n",
        "    ])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHODsfyG9H0"
      },
      "source": [
        "#Let's see how this affects the shape of an input\n",
        "R = tf.random.uniform(shape=(16, 224, 224, 3))\n",
        "for layer in model_one().layers:\n",
        "    R = layer(R)\n",
        "    print(layer.__class__.__name__, 'output shape:\\t', R.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5I0GBdsHMD-"
      },
      "source": [
        "We need to specify which loss function to use and which optimizer to use.  This is a multi-label classification problem, which means softmax is NOT the appropriate choice, instead each individual output node has a logistic loss associated with it, so the output of a particular node corresponds to the probability of that trait (ex. glaucoma) being present in the image.  There is one catch: one of the categories is \"Normal\", which by definition is mutually exclusive with the combined set of the other six categories. Ideally there would be a way to enforce unitarity, that is, Prob(Normal) + Prob(Not Normal) = 1, but it is not clear at this point how to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK14R5sQHckF"
      },
      "source": [
        "It is generally known that learning rate decay improves performance, the basic logic being that the model is unlikely to start near the global optimum so large steps are taken initially, then as it approaches the optimum step size reduces so as to not overshoot.  Many different schemes for learning rate decay have been proposed, for this project we are using the simple Exponential Decay, which Keras has as a built-in.  There are three hyperparameters associated with this scheduler: initial_learning_rate, decay_steps, and decay_rate.  The initial_learning_rate is self-explanatory and is also the one which is most variable in different experiments, the value of 3e-4 being on the smaller side relative to what I have seen others use.  Given more time I would test out other settings, probably going larger first (ex. ilr = 0.01).  The decay_steps and decay_rate parameters are set to what I believe are their commonly-used values, I would only manipulate these after other avenues of optimization have been exhausted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8-EzfzVHQcV"
      },
      "source": [
        "#Scheduler for the learning rate decay.  \n",
        "#Hyperparameters: initial_learning_rate, decay_steps, decay_rate\n",
        "irl = 3e-4\n",
        "dsteps = 10000\n",
        "drate = 0.9\n",
        "lr_scheduler = ExponentialDecay(\n",
        "    initial_learning_rate=irl,\n",
        "    decay_steps=dsteps,\n",
        "    decay_rate=drate)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx0LeuSpHnrX"
      },
      "source": [
        "#Set the number of training epochs\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TazOilMrH11n"
      },
      "source": [
        "Model_One = model_one()\n",
        "Model_One.compile(optimizer=Adam(learning_rate=lr_scheduler, epsilon=1e-8),\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=[tf.keras.metrics.Recall(), 'binary_accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWb_uE0OH-nY"
      },
      "source": [
        "M1_history = Model_One.fit(train_generator,\n",
        "                           validation_data = (X_test, y_test),\n",
        "                           epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}